/*@targets
 ** $maxopt baseline
 ** sse2 sse41
 ** vsx2
 ** neon asimd
 **/

/**
 * We ran into lots of test failures trying to enable this file for
 * VSE and VE on s390x (qemu) so avoiding these targets for now.
*/

/**
 * Force use SSE only on x86, even if AVX2 or AVX512F are enabled
 * through the baseline, since scatter(AVX512F) and gather very costly
 * to handle non-contiguous memory access comparing with SSE for
 * such small operations that this file covers.
*/
#define NPY_SIMD_FORCE_128
#define _UMATHMODULE
#define _MULTIARRAYMODULE
#define NPY_NO_DEPRECATED_API NPY_API_VERSION
#include <float.h>
#include "numpy/npy_math.h"
#include "simd/simd.h"
#include "loops_utils.h"
#include "loops.h"
#include "lowlevel_strided_loops.h"
// Provides the various *_LOOP macros
#include "fast_loop_macros.h"
/**********************************************************
 ** Scalars
 **********************************************************/
#if !NPY_SIMD_F32
NPY_FINLINE float c_recip_f32(float a)
{ return 1.0f / a; }
NPY_FINLINE float c_abs_f32(float a)
{
    const float tmp = a > 0 ? a : -a;
    /* add 0 to clear -0.0 */
    return tmp + 0;
}
NPY_FINLINE float c_square_f32(float a)
{ return a * a; }
#endif // !NPY_SIMD_F32

#if !NPY_SIMD_F64
NPY_FINLINE double c_recip_f64(double a)
{ return 1.0 / a; }
NPY_FINLINE double c_abs_f64(double a)
{
    const double tmp = a > 0 ? a : -a;
    /* add 0 to clear -0.0 */
    return tmp + 0;
}
NPY_FINLINE double c_square_f64(double a)
{ return a * a; }
#endif // !NPY_SIMD_F64
/**
 * MSVC(32-bit mode) requires a clarified contiguous loop
 * in order to use SSE, otherwise it uses a soft version of square root
 * that doesn't raise a domain error.
 */
#if defined(_MSC_VER) && defined(_M_IX86) && !NPY_SIMD
    #include <emmintrin.h>
    NPY_FINLINE float c_sqrt_f32(float _a)
    {
        __m128 a = _mm_load_ss(&_a);
        __m128 lower = _mm_sqrt_ss(a);
        return _mm_cvtss_f32(lower);
    }
    NPY_FINLINE double c_sqrt_f64(double _a)
    {
        __m128d a = _mm_load_sd(&_a);
        __m128d lower = _mm_sqrt_pd(a);
        return _mm_cvtsd_f64(lower);
    }
#else
    #define c_sqrt_f32 npy_sqrtf
    #define c_sqrt_f64 npy_sqrt
#endif

#define c_ceil_f32 npy_ceilf
#define c_ceil_f64 npy_ceil

#define c_trunc_f32 npy_truncf
#define c_trunc_f64 npy_trunc

#define c_floor_f32 npy_floorf
#define c_floor_f64 npy_floor

#define c_rint_f32 npy_rintf
#define c_rint_f64 npy_rint

/*******************************************************************************
 ** extra SIMD intrinsics
 ******************************************************************************/

#if NPY_SIMD
/**begin repeat
 * #TYPE = FLOAT, DOUBLE#
 * #sfx  = f32, f64#
 * #VCHK = NPY_SIMD_F32, NPY_SIMD_F64#
 * #FDMAX = FLT_MAX, DBL_MAX#
 * #fd = f, #
 * #ssfx = 32, 64#
 */
#if @VCHK@

static NPY_INLINE npyv_u@ssfx@
npyv_isnan_@sfx@(npyv_@sfx@ v)
{
    // (v != v) >> (size - 1)
#if defined(NPY_HAVE_SSE2) || defined (NPY_HAVE_SSE41)
    // sse npyv_cmpneq_@sfx@ define includes a cast already
    npyv_u@ssfx@ r = npyv_cmpneq_@sfx@(v, v);
#else
    npyv_u@ssfx@ r = npyv_cvt_u@ssfx@_b@ssfx@(npyv_cmpneq_@sfx@(v, v));
#endif
    return npyv_shri_u@ssfx@(r, (sizeof(npyv_lanetype_@sfx@)*8)-1);
}

static NPY_INLINE npyv_u@ssfx@
npyv_isinf_@sfx@(npyv_@sfx@ v)
{
    // (abs(v) > fltmax) >> (size - 1)
    const npyv_@sfx@ fltmax = npyv_setall_@sfx@(@FDMAX@);
#if defined(NPY_HAVE_NEON)
    npyv_u@ssfx@ r = vcagtq_@sfx@(v, fltmax);
#else
    // fabs via masking of sign bit
    const npyv_@sfx@ signmask = npyv_setall_@sfx@(-0.@fd@);
    npyv_u8      r_u8 = npyv_andc_u8(npyv_reinterpret_u8_@sfx@(v), npyv_reinterpret_u8_@sfx@(signmask));
 #if defined(NPY_HAVE_SSE2) || defined (NPY_HAVE_SSE41)
    // return cast already done in npyv_cmpgt_@sfx@
    npyv_u@ssfx@ r    = npyv_cmpgt_@sfx@(npyv_reinterpret_@sfx@_u8(r_u8), fltmax);
 #else
    npyv_u@ssfx@ r    = npyv_reinterpret_u@ssfx@_@sfx@(npyv_cmpgt_@sfx@(npyv_reinterpret_@sfx@_u8(r_u8), fltmax));
 #endif
#endif
    return npyv_shri_u@ssfx@(r, (sizeof(npyv_lanetype_@sfx@)*8)-1);
}

static NPY_INLINE npyv_u@ssfx@
npyv_isfinite_@sfx@(npyv_@sfx@ v)
{
    // ((v & signmask) <= fltmax) >> (size-1)
    const npyv_@sfx@ fltmax = npyv_setall_@sfx@(@FDMAX@);
#if defined(NPY_HAVE_NEON)
    npyv_u@ssfx@ r = vcaleq_@sfx@(v, fltmax);
#else
    // fabs via masking of sign bit
    const npyv_@sfx@ signmask = npyv_setall_@sfx@(-0.@fd@);
    npyv_u8      r_u8 = npyv_andc_u8(npyv_reinterpret_u8_@sfx@(v), npyv_reinterpret_u8_@sfx@(signmask));
 #if defined(NPY_HAVE_SSE2) || defined (NPY_HAVE_SSE41)
    // return cast already done in npyv_cmpgt_@sfx@
    npyv_u@ssfx@ r    = npyv_cmple_@sfx@(npyv_reinterpret_@sfx@_u8(r_u8), fltmax);
 #else
    npyv_u@ssfx@ r    = npyv_reinterpret_u@ssfx@_@sfx@(npyv_cmple_@sfx@(npyv_reinterpret_@sfx@_u8(r_u8), fltmax));
 #endif
#endif
    return npyv_shri_u@ssfx@(r, (sizeof(npyv_lanetype_@sfx@)*8)-1);
}

static NPY_INLINE npyv_u@ssfx@
npyv_signbit_@sfx@(npyv_@sfx@ v)
{
    return npyv_shri_u@ssfx@(npyv_reinterpret_u@ssfx@_@sfx@(v), (sizeof(npyv_lanetype_@sfx@)*8)-1);
}

#endif // @VCHK@
/**end repeat**/

// In these functions we use vqtbl4q_u8 which is only available on aarch64
#if defined(NPY_HAVE_NEON) && defined(__aarch64__)
    #define PREPACK_ISFINITE 1
    #define PREPACK_SIGNBIT  1

    #if NPY_SIMD_F32

    static NPY_INLINE npyv_u8
    npyv_isfinite_4x_f32(npyv_f32 v0, npyv_f32 v1, npyv_f32 v2, npyv_f32 v3)
    {
        // F32 exponent is 8-bits, which means we can pack multiple into
        // a single vector.  We shift out sign bit so that we're left
        // with only exponent in high byte.  If not all bits are set,
        // then we've got a finite number.
        uint8x16x4_t tbl;
        tbl.val[0] = npyv_reinterpret_u8_u32(npyv_shli_u32(npyv_reinterpret_u32_f32(v0), 1));
        tbl.val[1] = npyv_reinterpret_u8_u32(npyv_shli_u32(npyv_reinterpret_u32_f32(v1), 1));
        tbl.val[2] = npyv_reinterpret_u8_u32(npyv_shli_u32(npyv_reinterpret_u32_f32(v2), 1));
        tbl.val[3] = npyv_reinterpret_u8_u32(npyv_shli_u32(npyv_reinterpret_u32_f32(v3), 1));

        const npyv_u8 permute = {3,7,11,15,  19,23,27,31,  35,39,43,47,  51,55,59,63};
        npyv_u8 r = vqtbl4q_u8(tbl, permute);

        const npyv_u8 expmask = npyv_setall_u8(0xff);
        r = npyv_cmpneq_u8(r, expmask);
        r = vshrq_n_u8(r, 7);
        return r;
    }

    static NPY_INLINE npyv_u8
    npyv_signbit_4x_f32(npyv_f32 v0, npyv_f32 v1, npyv_f32 v2, npyv_f32 v3)
    {
        // We only need high byte for signbit, which means we can pack
        // multiple inputs into a single vector.
        uint8x16x4_t tbl;
        tbl.val[0] = npyv_reinterpret_u8_f32(v0);
        tbl.val[1] = npyv_reinterpret_u8_f32(v1);
        tbl.val[2] = npyv_reinterpret_u8_f32(v2);
        tbl.val[3] = npyv_reinterpret_u8_f32(v3);

        const npyv_u8 permute = {3,7,11,15,  19,23,27,31,  35,39,43,47,  51,55,59,63};
        npyv_u8 r = vqtbl4q_u8(tbl, permute);
                r = vshrq_n_u8(r, 7);
        return r;
    }

    #endif // NPY_SIMD_F32

    #if NPY_SIMD_F64

    static NPY_INLINE npyv_u8
    npyv_isfinite_8x_f64(npyv_f64 v0, npyv_f64 v1, npyv_f64 v2, npyv_f64 v3,
                         npyv_f64 v4, npyv_f64 v5, npyv_f64 v6, npyv_f64 v7)
    {
        // F64 exponent is 11-bits, which means we can pack multiple into
        // a single vector.  We'll need to use u16 to fit all exponent
        // bits.  If not all bits are set, then we've got a finite number.
        uint8x16x4_t t0123, t4567;
        t0123.val[0] = npyv_reinterpret_u8_f64(v0);
        t0123.val[1] = npyv_reinterpret_u8_f64(v1);
        t0123.val[2] = npyv_reinterpret_u8_f64(v2);
        t0123.val[3] = npyv_reinterpret_u8_f64(v3);
        t4567.val[0] = npyv_reinterpret_u8_f64(v4);
        t4567.val[1] = npyv_reinterpret_u8_f64(v5);
        t4567.val[2] = npyv_reinterpret_u8_f64(v6);
        t4567.val[3] = npyv_reinterpret_u8_f64(v7);

        const npyv_u8 permute = {6,7,14,15,  22,23,30,31,  38,39,46,47,  54,55,62,63};
        npyv_u16 r0 = npyv_reinterpret_u16_u8(vqtbl4q_u8(t0123, permute));
        npyv_u16 r1 = npyv_reinterpret_u16_u8(vqtbl4q_u8(t4567, permute));

        const npyv_u16 expmask = npyv_setall_u16(0x7ff0);
        r0 = npyv_and_u16(r0, expmask);
        r0 = npyv_cmpneq_u16(r0, expmask);
        r0 = npyv_shri_u16(r0, 15);
        r1 = npyv_and_u16(r1, expmask);
        r1 = npyv_cmpneq_u16(r1, expmask);
        r1 = npyv_shri_u16(r1, 15);

        npyv_u8 r = npyv_pack_b8_b16(r0, r1);
        return r;
    }

    static NPY_INLINE npyv_u8
    npyv_signbit_8x_f64(npyv_f64 v0, npyv_f64 v1, npyv_f64 v2, npyv_f64 v3,
                        npyv_f64 v4, npyv_f64 v5, npyv_f64 v6, npyv_f64 v7)
    {
        // We only need high byte for signbit, which means we can pack
        // multiple inputs into a single vector.

        // vuzp2 faster than vtbl for f64
        npyv_u32 v01 = vuzp2q_u32(npyv_reinterpret_u32_f64(v0), npyv_reinterpret_u32_f64(v1));
        npyv_u32 v23 = vuzp2q_u32(npyv_reinterpret_u32_f64(v2), npyv_reinterpret_u32_f64(v3));
        npyv_u32 v45 = vuzp2q_u32(npyv_reinterpret_u32_f64(v4), npyv_reinterpret_u32_f64(v5));
        npyv_u32 v67 = vuzp2q_u32(npyv_reinterpret_u32_f64(v6), npyv_reinterpret_u32_f64(v7));

        npyv_u16 v0123 = vuzp2q_u16(npyv_reinterpret_u16_u32(v01), npyv_reinterpret_u16_u32(v23));
        npyv_u16 v4567 = vuzp2q_u16(npyv_reinterpret_u16_u32(v45), npyv_reinterpret_u16_u32(v67));

        npyv_u8 r = vuzp2q_u8(npyv_reinterpret_u8_u16(v0123), npyv_reinterpret_u8_u16(v4567));
                r = vshrq_n_u8(r, 7);
        return r;
    }

    #endif // NPY_SIMD_F64

#else
    #define PREPACK_ISFINITE 0
    #define PREPACK_SIGNBIT  0
#endif // defined(NPY_HAVE_NEON) && defined(__aarch64__)

#endif // NPY_SIMD

/********************************************************************************
 ** Defining the SIMD kernels
 ********************************************************************************/
/** Notes:
 * - avoid the use of libmath to unify fp/domain errors
 *   for both scalars and vectors among all compilers/architectures.
 * - use intrinsic npyv_load_till_* instead of npyv_load_tillz_
 *   to fill the remind lanes with 1.0 to avoid divide by zero fp
 *   exception in reciprocal.
 */
#define CONTIG  0
#define NCONTIG 1

/*
 * clang has a bug that's present at -O1 or greater.  When partially loading a
 * vector register for a reciprocal operation, the remaining elements are set
 * to 1 to avoid divide-by-zero.  The partial load is paired with a partial
 * store after the reciprocal operation.  clang notices that the entire register
 * is not needed for the store and optimizes out the fill of 1 to the remaining
 * elements.  This causes either a divide-by-zero or 0/0 with invalid exception
 * that we were trying to avoid by filling.
 *
 * Using a dummy variable marked 'volatile' convinces clang not to ignore
 * the explicit fill of remaining elements.  If `-ftrapping-math` is
 * supported, then it'll also avoid the bug.  `-ftrapping-math` is supported
 * on Apple clang v12+ for x86_64.  It is not currently supported for arm64.
 * `-ftrapping-math` is set by default of Numpy builds in
 * numpy/distutils/ccompiler.py.
 *
 * Note: Apple clang and clang upstream have different versions that overlap
 */
#if defined(__clang__)
    #if defined(__apple_build_version__)
    // Apple Clang
        #if __apple_build_version__ < 12000000
        // Apple Clang before v12
        #define WORKAROUND_CLANG_RECIPROCAL_BUG 1
        #elif defined(NPY_CPU_X86) || defined(NPY_CPU_AMD64)
        // Apple Clang after v12, targeting i386 or x86_64
        #define WORKAROUND_CLANG_RECIPROCAL_BUG 0
        #else
        // Apple Clang after v12, not targeting i386 or x86_64
        #define WORKAROUND_CLANG_RECIPROCAL_BUG 1
        #endif
    #else
    // Clang, not Apple Clang
        #if __clang_major__ < 10
        // Clang before v10
        #define WORKAROUND_CLANG_RECIPROCAL_BUG 1
        #elif defined(_MSC_VER)
        // clang-cl has the same bug
        #define WORKAROUND_CLANG_RECIPROCAL_BUG 1
        #elif defined(NPY_CPU_X86) || defined(NPY_CPU_AMD64)
        // Clang v10+, targeting i386 or x86_64
        #define WORKAROUND_CLANG_RECIPROCAL_BUG 0
        #else
        // Clang v10+, not targeting i386 or x86_64
        #define WORKAROUND_CLANG_RECIPROCAL_BUG 1
        #endif
    #endif
#else
// Not a Clang compiler
#define WORKAROUND_CLANG_RECIPROCAL_BUG 0
#endif

/**begin repeat
 * #TYPE = FLOAT, DOUBLE#
 * #sfx  = f32, f64#
 * #VCHK = NPY_SIMD_F32, NPY_SIMD_F64#
 * #FDMAX = FLT_MAX, DBL_MAX#
 * #fd = f, #
 * #ssfx = 32, 64#
 */
#if @VCHK@
/**begin repeat1
 * #kind     = rint,  floor, ceil, trunc, sqrt, absolute, square, reciprocal#
 * #intr     = rint,  floor, ceil, trunc, sqrt, abs,      square, recip#
 * #repl_0w1 = 0*7, 1#
 * #RECIP_WORKAROUND = 0*7, WORKAROUND_CLANG_RECIPROCAL_BUG#
 */
/**begin repeat2
 * #STYPE  = CONTIG, NCONTIG, CONTIG,  NCONTIG#
 * #DTYPE  = CONTIG, CONTIG,  NCONTIG, NCONTIG#
 * #unroll = 4,      4,       2,       2#
 */
static void simd_@TYPE@_@kind@_@STYPE@_@DTYPE@
(const void *_src, npy_intp ssrc, void *_dst, npy_intp sdst, npy_intp len)
{
    const npyv_lanetype_@sfx@ *src = _src;
          npyv_lanetype_@sfx@ *dst = _dst;

    const int vstep = npyv_nlanes_@sfx@;
    const int wstep = vstep * @unroll@;

    // unrolled iterations
    for (; len >= wstep; len -= wstep, src += ssrc*wstep, dst += sdst*wstep) {
        /**begin repeat3
         * #N  = 0, 1, 2, 3#
         */
        #if @unroll@ > @N@
            #if @STYPE@ == CONTIG
                npyv_@sfx@ v_src@N@ = npyv_load_@sfx@(src + vstep*@N@);
            #else
                npyv_@sfx@ v_src@N@ = npyv_loadn_@sfx@(src + ssrc*vstep*@N@, ssrc);
            #endif
            npyv_@sfx@ v_unary@N@ = npyv_@intr@_@sfx@(v_src@N@);
        #endif
        /**end repeat3**/
        /**begin repeat3
         * #N  = 0, 1, 2, 3#
         */
        #if @unroll@ > @N@
            #if @DTYPE@ == CONTIG
                npyv_store_@sfx@(dst + vstep*@N@, v_unary@N@);
            #else
                npyv_storen_@sfx@(dst + sdst*vstep*@N@, sdst, v_unary@N@);
            #endif
        #endif
        /**end repeat3**/
    }

    // vector-sized iterations
    for (; len >= vstep; len -= vstep, src += ssrc*vstep, dst += sdst*vstep) {
    #if @STYPE@ == CONTIG
        npyv_@sfx@ v_src0 = npyv_load_@sfx@(src);
    #else
        npyv_@sfx@ v_src0 = npyv_loadn_@sfx@(src, ssrc);
    #endif
        npyv_@sfx@ v_unary0 = npyv_@intr@_@sfx@(v_src0);
    #if @DTYPE@ == CONTIG
        npyv_store_@sfx@(dst, v_unary0);
    #else
        npyv_storen_@sfx@(dst, sdst, v_unary0);
    #endif
    }

    // last partial iteration, if needed
    if(len > 0){
    #if @STYPE@ == CONTIG
        #if @repl_0w1@
            npyv_@sfx@ v_src0 = npyv_load_till_@sfx@(src, len, 1);
        #else
            npyv_@sfx@ v_src0 = npyv_load_tillz_@sfx@(src, len);
        #endif
    #else
        #if @repl_0w1@
            npyv_@sfx@ v_src0 = npyv_loadn_till_@sfx@(src, ssrc, len, 1);
        #else
            npyv_@sfx@ v_src0 = npyv_loadn_tillz_@sfx@(src, ssrc, len);
        #endif
    #endif
        #if @RECIP_WORKAROUND@
            /*
             * Workaround clang bug.  We use a dummy variable marked 'volatile'
             * to convince clang that the entire vector is needed.  We only
             * want to do this for the last iteration / partial load-store of
             * the loop since 'volatile' forces a refresh of the contents.
             */
             volatile npyv_@sfx@ unused_but_workaround_bug = v_src0;
        #endif // @RECIP_WORKAROUND@
        npyv_@sfx@ v_unary0 = npyv_@intr@_@sfx@(v_src0);
    #if @DTYPE@ == CONTIG
        npyv_store_till_@sfx@(dst, len, v_unary0);
    #else
        npyv_storen_till_@sfx@(dst, sdst, len, v_unary0);
    #endif
    }

    npyv_cleanup();
}
/**end repeat2**/
/**end repeat1**/

/**begin repeat1
 * #kind = isnan, isinf, isfinite, signbit#
 * #PREPACK = 0, 0, PREPACK_ISFINITE, PREPACK_SIGNBIT#
 */
/**begin repeat2
 * #STYPE  = CONTIG, NCONTIG, CONTIG,  NCONTIG#
 * #DTYPE  = CONTIG, CONTIG,  NCONTIG, NCONTIG#
 * #unroll = 1, 1, 1, 1#
 */
static void simd_unary_@kind@_@TYPE@_@STYPE@_@DTYPE@
(const void *src, npy_intp istride, void *dst, npy_intp ostride, npy_intp len)
{
    const npyv_lanetype_@sfx@ *ip = src;
    npy_bool *op = dst;

    // How many vectors can be packed into a u8 / bool vector?
    #define PACK_FACTOR (NPY_SIMD_WIDTH / npyv_nlanes_@sfx@)
    assert(PACK_FACTOR == 4 || PACK_FACTOR == 8);

    const int vstep = npyv_nlanes_@sfx@;
    const int wstep = vstep * @unroll@ * PACK_FACTOR;

    // unrolled iterations
    for (; len >= wstep; len -= wstep, ip += istride*wstep, op += ostride*wstep) {
        /**begin repeat3
         * #N  = 0, 1, 2, 3#
         */
        #if @unroll@ > @N@
            // Load vectors
            #if @STYPE@ == CONTIG
                // contiguous input
                npyv_@sfx@ v0_@N@ = npyv_load_@sfx@(ip + vstep * (0 + PACK_FACTOR * @N@)); // 2 * (0 + 8 * n)
                npyv_@sfx@ v1_@N@ = npyv_load_@sfx@(ip + vstep * (1 + PACK_FACTOR * @N@));
                npyv_@sfx@ v2_@N@ = npyv_load_@sfx@(ip + vstep * (2 + PACK_FACTOR * @N@));
                npyv_@sfx@ v3_@N@ = npyv_load_@sfx@(ip + vstep * (3 + PACK_FACTOR * @N@));
                #if PACK_FACTOR == 8
                npyv_@sfx@ v4_@N@ = npyv_load_@sfx@(ip + vstep * (4 + PACK_FACTOR * @N@));
                npyv_@sfx@ v5_@N@ = npyv_load_@sfx@(ip + vstep * (5 + PACK_FACTOR * @N@));
                npyv_@sfx@ v6_@N@ = npyv_load_@sfx@(ip + vstep * (6 + PACK_FACTOR * @N@));
                npyv_@sfx@ v7_@N@ = npyv_load_@sfx@(ip + vstep * (7 + PACK_FACTOR * @N@)); // 2 * (7 + 8 * n) --> 32 + 7: 39 * 2: 78
                #endif
            #else
                // non-contiguous input
                npyv_@sfx@ v0_@N@ = npyv_loadn_@sfx@(ip + istride*vstep*(0 + PACK_FACTOR * @N@), istride);
                npyv_@sfx@ v1_@N@ = npyv_loadn_@sfx@(ip + istride*vstep*(1 + PACK_FACTOR * @N@), istride);
                npyv_@sfx@ v2_@N@ = npyv_loadn_@sfx@(ip + istride*vstep*(2 + PACK_FACTOR * @N@), istride);
                npyv_@sfx@ v3_@N@ = npyv_loadn_@sfx@(ip + istride*vstep*(3 + PACK_FACTOR * @N@), istride);
                #if PACK_FACTOR == 8
                npyv_@sfx@ v4_@N@ = npyv_loadn_@sfx@(ip + istride*vstep*(4 + PACK_FACTOR * @N@), istride);
                npyv_@sfx@ v5_@N@ = npyv_loadn_@sfx@(ip + istride*vstep*(5 + PACK_FACTOR * @N@), istride);
                npyv_@sfx@ v6_@N@ = npyv_loadn_@sfx@(ip + istride*vstep*(6 + PACK_FACTOR * @N@), istride);
                npyv_@sfx@ v7_@N@ = npyv_loadn_@sfx@(ip + istride*vstep*(7 + PACK_FACTOR * @N@), istride);
                #endif
            #endif
        #endif // @unroll@ > @N@
        /**end repeat3**/

        /**begin repeat3
         * #N  = 0, 1, 2, 3#
         */
        #if @unroll@ > @N@
        #if @PREPACK@ && (@ssfx@ == 32 || @ssfx@ == 64)
            #if @ssfx@ == 32
            npyv_u8 r_@N@ = npyv_@kind@_4x_@sfx@(v0_@N@, v1_@N@, v2_@N@, v3_@N@);
            #elif @ssfx@ == 64
            npyv_u8 r_@N@ = npyv_@kind@_8x_@sfx@(v0_@N@, v1_@N@, v2_@N@, v3_@N@,
                                                 v4_@N@, v5_@N@, v6_@N@, v7_@N@);
            #endif
        #else
            npyv_b@ssfx@ r0_@N@ = npyv_cvt_b@ssfx@_u@ssfx@(npyv_@kind@_@sfx@(v0_@N@));
            npyv_b@ssfx@ r1_@N@ = npyv_cvt_b@ssfx@_u@ssfx@(npyv_@kind@_@sfx@(v1_@N@));
            npyv_b@ssfx@ r2_@N@ = npyv_cvt_b@ssfx@_u@ssfx@(npyv_@kind@_@sfx@(v2_@N@));
            npyv_b@ssfx@ r3_@N@ = npyv_cvt_b@ssfx@_u@ssfx@(npyv_@kind@_@sfx@(v3_@N@));
            #if PACK_FACTOR == 8
            npyv_b@ssfx@ r4_@N@ = npyv_cvt_b@ssfx@_u@ssfx@(npyv_@kind@_@sfx@(v4_@N@));
            npyv_b@ssfx@ r5_@N@ = npyv_cvt_b@ssfx@_u@ssfx@(npyv_@kind@_@sfx@(v5_@N@));
            npyv_b@ssfx@ r6_@N@ = npyv_cvt_b@ssfx@_u@ssfx@(npyv_@kind@_@sfx@(v6_@N@));
            npyv_b@ssfx@ r7_@N@ = npyv_cvt_b@ssfx@_u@ssfx@(npyv_@kind@_@sfx@(v7_@N@));
            #endif // PACK_FACTOR == 8
        #endif // @PREPACK@ && (@ssfx@ == 32 || @ssfx@ == 64)
        #endif // @unroll@ > @N@
        /**end repeat3**/

        /**begin repeat3
         * #N  = 0, 1, 2, 3#
         */
        #if @unroll@ > @N@
        #if @DTYPE@ == CONTIG
            #if @PREPACK@ && (@ssfx@ == 32 || @ssfx@ == 64)
                // Nothing to do, results already packed
            #else
                // Pack results
                #if PACK_FACTOR == 4
                npyv_u8 r_@N@ = npyv_cvt_u8_b8(npyv_pack_b8_b32(r0_@N@, r1_@N@, r2_@N@, r3_@N@));
                #elif PACK_FACTOR == 8
                npyv_u8 r_@N@ = npyv_cvt_u8_b8(npyv_pack_b8_b64(r0_@N@, r1_@N@, r2_@N@, r3_@N@,
                                                                r4_@N@, r5_@N@, r6_@N@, r7_@N@));
                #endif // PACK_FACTOR == 8
            #endif // @PREPACK@ && (@ssfx@ == 32 || @ssfx@ == 64)

            npyv_store_u8(op + vstep * PACK_FACTOR * @N@, r_@N@);

        #else // @DTYPE@ == CONTIG
            #if @PREPACK@ && (@ssfx@ == 32 || @ssfx@ == 64)
                // Results are packed, so we can just loop over them
                npy_uint8 lane_@N@[npyv_nlanes_u8];
                npyv_store_u8(lane_@N@, r_@N@);
                for (int ln=0; (ln * sizeof(npyv_lanetype_@sfx@)) < npyv_nlanes_u8; ++ln){
                    op[(ln + @N@ * PACK_FACTOR * vstep) * ostride] = lane_@N@[ln * sizeof(npyv_lanetype_@sfx@)];
                }
            #else
                // Results are not packed.  Use template to loop.
                /**begin repeat4
                 * #R = 0, 1, 2, 3, 4, 5, 6, 7#
                 */
                #if @R@ < PACK_FACTOR
                npy_uint8 lane@R@_@N@[npyv_nlanes_u8];
                npyv_store_u8(lane@R@_@N@, npyv_reinterpret_u8_u@ssfx@(r@R@_@N@));
                op[(0 + (@R@ + @N@ * PACK_FACTOR) * vstep) * ostride] = lane@R@_@N@[0 * sizeof(npyv_lanetype_@sfx@)];
                op[(1 + (@R@ + @N@ * PACK_FACTOR) * vstep) * ostride] = lane@R@_@N@[1 * sizeof(npyv_lanetype_@sfx@)];
                #if npyv_nlanes_@sfx@ == 4
                op[(2 + (@R@ + @N@ * PACK_FACTOR) * vstep) * ostride] = lane@R@_@N@[2 * sizeof(npyv_lanetype_@sfx@)];
                op[(3 + (@R@ + @N@ * PACK_FACTOR) * vstep) * ostride] = lane@R@_@N@[3 * sizeof(npyv_lanetype_@sfx@)];
                #endif
                #endif
                /**end repeat4**/
            #endif // @PREPACK@ && (@ssfx@ == 32 || @ssfx@ == 64)
        #endif // @DTYPE@ == CONTIG
        #endif // @unroll@ > @N@
        /**end repeat3**/
    }

    // vector-sized iterations
    for (; len >= vstep; len -= vstep, ip += istride*vstep, op += ostride*vstep) {
    #if @STYPE@ == CONTIG
        npyv_@sfx@ v = npyv_load_@sfx@(ip);
    #else
        npyv_@sfx@ v = npyv_loadn_@sfx@(ip, istride);
    #endif

        npyv_u@ssfx@ r = npyv_@kind@_@sfx@(v);

        npy_uint8 lane[npyv_nlanes_u8];
        npyv_store_u8(lane, npyv_reinterpret_u8_u@ssfx@(r));

        op[0*ostride] = lane[0 * sizeof(npyv_lanetype_@sfx@)];
        op[1*ostride] = lane[1 * sizeof(npyv_lanetype_@sfx@)];
        #if npyv_nlanes_@sfx@ == 4
        op[2*ostride] = lane[2 * sizeof(npyv_lanetype_@sfx@)];
        op[3*ostride] = lane[3 * sizeof(npyv_lanetype_@sfx@)];
        #endif
    }

    #undef PACK_FACTOR

    // Scalar loop to finish off
    for (; len > 0; --len, ip += istride, op += ostride) {
        *op = (npy_@kind@(*ip) != 0);
    }


    npyv_cleanup();
}
/**end repeat2**/
/**end repeat1**/

#endif // @VCHK@
/**end repeat**/

#undef WORKAROUND_CLANG_RECIPROCAL_BUG
#undef PREPACK_ISFINITE
#undef PREPACK_SIGNBIT

/********************************************************************************
 ** Defining ufunc inner functions
 ********************************************************************************/
/**begin repeat
 * #TYPE = FLOAT, DOUBLE#
 * #sfx  = f32, f64#
 * #VCHK = NPY_SIMD_F32, NPY_SIMD_F64#
 */
/**begin repeat1
 * #kind  = rint, floor, ceil, trunc, sqrt, absolute, square, reciprocal#
 * #intr  = rint, floor, ceil, trunc, sqrt, abs,      square, recip#
 * #clear = 0,    0,     0,    0,     0,    1,        0,      0#
 */
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
    const char *src = args[0]; char *dst = args[1];
    const npy_intp src_step = steps[0];
    const npy_intp dst_step = steps[1];
    npy_intp len = dimensions[0];
#if @VCHK@
    const int lsize = sizeof(npyv_lanetype_@sfx@);
    assert(len <= 1 || (src_step % lsize == 0 && dst_step % lsize == 0));
    if (is_mem_overlap(src, src_step, dst, dst_step, len)) {
        goto no_unroll;
    }
    const npy_intp ssrc = src_step / lsize;
    const npy_intp sdst = dst_step / lsize;
    if (!npyv_loadable_stride_@sfx@(ssrc) || !npyv_storable_stride_@sfx@(sdst)) {
        goto no_unroll;
    }
    if (ssrc == 1 && sdst == 1) {
        simd_@TYPE@_@kind@_CONTIG_CONTIG(src, 1, dst, 1, len);
    }
    else if (sdst == 1) {
        simd_@TYPE@_@kind@_NCONTIG_CONTIG(src, ssrc, dst, 1, len);
    }
    else if (ssrc == 1) {
        simd_@TYPE@_@kind@_CONTIG_NCONTIG(src, 1, dst, sdst, len);
    } else {
        simd_@TYPE@_@kind@_NCONTIG_NCONTIG(src, ssrc, dst, sdst, len);
    }
    goto clear;
no_unroll:
#endif // @VCHK@
    for (; len > 0; --len, src += src_step, dst += dst_step) {
    #if @VCHK@
        // to guarantee the same precision and fp/domain errors for both scalars and vectors
        simd_@TYPE@_@kind@_CONTIG_CONTIG(src, 0, dst, 0, 1);
    #else
        const npyv_lanetype_@sfx@ src0 = *(npyv_lanetype_@sfx@*)src;
        *(npyv_lanetype_@sfx@*)dst = c_@intr@_@sfx@(src0);
    #endif
    }
#if @VCHK@
clear:;
#endif
#if @clear@
    npy_clear_floatstatus_barrier((char*)dimensions);
#endif
}
/**end repeat1**/

/**begin repeat1
 * #kind = isnan, isinf, isfinite, signbit#
 **/
NPY_NO_EXPORT void NPY_CPU_DISPATCH_CURFX(@TYPE@_@kind@)
(char **args, npy_intp const *dimensions, npy_intp const *steps, void *NPY_UNUSED(func))
{
#if @VCHK@
    const char *ip = args[0];
    char *op = args[1];
    const npy_intp istep = steps[0];
    const npy_intp ostep = steps[1];
    npy_intp len = dimensions[0];
    const int ilsize = sizeof(npyv_lanetype_@sfx@);
    const int olsize = sizeof(npy_bool);
    const npy_intp istride = istep / ilsize;
    const npy_intp ostride = ostep / olsize;
    assert(len <= 1 || ostep % olsize == 0);

    if ((istep % ilsize == 0) &&
        !is_mem_overlap(ip, istep, op, ostep, len) &&
        npyv_loadable_stride_@sfx@(istride) &&
        npyv_storable_stride_@sfx@(ostride))
    {
        if (istride == 1 && ostride == 1) {
            simd_unary_@kind@_@TYPE@_CONTIG_CONTIG(ip, 1, op, 1, len);
        }
        else if (ostride == 1) {
            simd_unary_@kind@_@TYPE@_NCONTIG_CONTIG(ip, istride, op, 1, len);
        }
        else if (istride == 1) {
            simd_unary_@kind@_@TYPE@_CONTIG_NCONTIG(ip, 1, op, ostride, len);
        } else {
            simd_unary_@kind@_@TYPE@_NCONTIG_NCONTIG(ip, istride, op, ostride, len);
        }
    } else
#endif // @VCHK@
    {
    UNARY_LOOP {
        const npyv_lanetype_@sfx@ in = *(npyv_lanetype_@sfx@ *)ip1;
        *((npy_bool *)op1) = (npy_@kind@(in) != 0);
    }
    }

    npy_clear_floatstatus_barrier((char*)dimensions);
}
/**end repeat1**/
/**end repeat**/
